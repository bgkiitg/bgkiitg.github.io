---
title: "Statistical Learning"
categories:
  - Blog
tags:
  - ISLP
  - Notes
  - ML
use_math: true
---
 What really is a learning problem from a statistical point of view?


<br>
Consider the function $$f(x)= x+ 2x^3$$. The python code  given below samples this function
10000 times from the range $$[-2,2]$$. The sampled value at $$x$$ is not exactly $$f(x)$$ but has a "noise" which is a normally distributed random variable with mean 0 and variance 1.
{% highlight python %}
import numpy as np

def f(x):
    return x + 2 * (x ** 3)

# Generate 10000 x values in the range [-2, 2]
x_values = np.random.uniform(-2, 2, 10000)
y_values = f(x_values) + np.random.normal(0, 1, 10000)

# Plot the points
plt.scatter(x_values, y_values, alpha=0.5, s=5, label="Noisy Data")
plt.xlabel("x values")
plt.ylabel("y values")
plt.title("Scatter Plot of Sampled Points")
plt.legend()
plt.show()

# Print the first 10 points as a sample
for i in range(10):
    print(f"Point on the curve: (x={x_values[i]:.4f}, y={y_values[i]:.4f})")



{% endhighlight %}


The scatter plot of the data generated by this program is shown below.

![Scatter Plot](/assets/images/Figure_1.png){: .center-image }

The statistical learning problem is :

Given just this scatter plot(data) , can you guess what the underlying function was?

With this data, can you deduce that the  $$f(x)= x+ 2x^3$$.  How much data do you need to make  this inference? If another person generated data using the same program, does her data also, when analysed give us the conclusion that the underlying function was  $$f(x)= x+ 2x^3$$?


In many real life situations, we just have the data. We do not know that the data was generated by a python code or some deterministic process plus some randomness. We abstract all these uncertainities to randomness and hope to recover the "true" function $$f$$ from the data.

Formally, $$Y$$ or the output variable or the dependent variable or the response is a random variable that has the following form 

$$Y = f(X) + \epsilon$$

where $$f$$ is an unknown but fixed function and $$\epsilon$$ is a random variable with mean 0 and $$X$$ is a vector of predictors or input variables. Note that the random variable $$Y$$ is defined on the same probability space as the random variable $$\epsilon$$. Further there is no randomness as such in $$X$$. More precisely, $$Y(X)$$ is a random variable for each $$X$$

 We are given a bunch of $$Y$$ samples and the corresponding $$X$$ and are asked to guess what is $$f$$. We may use this data and use statistical learning teachniques and come up with suitable choices for $$f$$. Let's denote this estimate function as $$ \hat{f}$$. We may use this to predict $$Y$$ at a point $$X$$ as $$\hat{f}(X)$$. The closer $$ \hat{f}$$. is to $$f$$, our estimation will be better. But even if we knew exactly what $$f$$ there is be error as $$Y$$ has a component from $$\epsilon$$.


Even if we knew exactly the process, most often we do not know, that generated the data, $$Y$$ is a random variable and thus will have a variance. Given $$\hat{f}$$ and $$X$$, our prediction will be  $$\hat{f}(X)$$.  Then we have the following:

$$ 
E(Y - \hat{Y})^2 = E\left[ f(X) + \epsilon - \hat{f}(X) \right]^2



= \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{Reducible}} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{Irreducible}}

$$


The first error term is called reducible because it can be reduced by giving a better $$\hat{f}$$. We can't do that for the irreducible component.


Knowing the exat form of  $$f$$ also helps us draw conclusions of the following kind:

 - Which input predictors are  important?
 - Which input predict causes maximum change?
 - For a given improvement in output, huch much should a certain predictor change?
 
 
 These questions are considered as "inference" related rather than  the predictor kind.
 
 
 Now consider the problem of determining $$\hat{f}$$. There are broadly two approaches:
 
 - Parametric
 
 Here we may assume that $$f$$ have a certain shape, say linear, and then estimates the parameter using the data
 using methods like OLS etc.
 
 - Non-parametric methods
 
 Here we just try to get as close a fit as possible without being restricted by say the shape. An example would be A thin-plate spline method. These approaches allow for a much wide collection of shapes. We choose a smoothness and then try to find a function satisyfying the smoothness criteria.
 
 Here is a list of statistical learning methods. The flexibility goes up  as we go down the list but the interpretability goes down as flexibility goes up.


1. Subset selection
2. Lasso
3. Least squares
4. Generalized Additive models
5. Trees
6. Bagging, Boosting
7. SVM
8. Deep Learning


The approaches may be classified as supervsed or Unsupervised on semisupervised on the basis of the type of data we have.
Further, on the basis of the values that $$Y$$ can take, we may think of the problem as being classification or regression.

Once we have computed $$\hat{f}$$, the quality of the fit can be quantified by computing the Mean Squared  Error on a test set. The  MSE on a test with $$n$$ sample points  is given by 



$$MSE= \displaystyle \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2$$
